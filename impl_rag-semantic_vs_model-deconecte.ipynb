{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fad27b-d38d-4bca-bd47-bc1f66219fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub > /dev/null 2>&1\n",
    "!pip install transformers > /dev/null 2>&1\n",
    "!pip install accelerate > /dev/null 2>&1\n",
    "!pip install sentencepiece > /dev/null 2>&1\n",
    "!pip install timm > /dev/null 2>&1\n",
    "!pip install einops > /dev/null 2>&1\n",
    "!pip install bitsandbytes accelerate > /dev/null 2>&1\n",
    "!pip install langchain langchain-community sentence-transformers chromadb pypdf unstructured pysqlite3-binary > /dev/null 2>&1\n",
    "!pip install langchain-experimental > /dev/null 2>&1\n",
    "!pip install minio > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb158e-e7d3-401c-bbb7-6ec1e0f4ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "import gc\n",
    "import requests\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import nltk\n",
    "import ssl\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules['pysqlite3']\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader # Pour charger des PDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.llms import HuggingFacePipeline # Pour envelopper votre modèle HF pour LangChain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d723c5-f7f7-4bd2-9f7a-fd63af4c5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26bd17-2a4c-4072-86ab-9ce82b374643",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data_dir = \"./nltk_data\"\n",
    "\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    print(f\"Le dossier '{nltk_data_dir}' est introuvable.\")\n",
    "    print(\"Tentative de téléchargement des paquets NLTK requis ('punkt', 'stopwords')...\")\n",
    "    \n",
    "    os.makedirs(nltk_data_dir)\n",
    "    \n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "    nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "    nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "    \n",
    "    print(\"Téléchargement des paquets NLTK terminé.\")\n",
    "else:\n",
    "    print(f\"Le dossier '{nltk_data_dir}' existe déjà. Aucune action de téléchargement n'est nécessaire.\")\n",
    "\n",
    "if os.path.abspath(nltk_data_dir) not in nltk.data.path:\n",
    "    nltk.data.path.append(os.path.abspath(nltk_data_dir))\n",
    "\n",
    "print(\"Le script est configuré pour utiliser les paquets NLTK en mode déconnecté.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb0cb7-1ee8-4024-953e-6d3cb1a08f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_BUCKET_NAME = \"reda-rag\"\n",
    "Access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "Secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "s3_endpoint = \"192.168.0.150:9000\"\n",
    "\n",
    "LOCAL_MODEL_PATH = \"Meta-Llama-3.2-3B-Instruct/\"\n",
    "LLM_LOCAL_PATH = \"./models/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "EMBEDDING_MINIO_PREFIX = \"all-MiniLM-L6-v2/\"\n",
    "LOCAL_MODEL_PATH_2 = \"./models/all-MiniLM-L6-v2\"\n",
    "\n",
    "def download_model_via_streaming(client, bucket, prefix, local_path):\n",
    "    \"\"\"\n",
    "    Télécharge les fichiers via streaming pour une utilisation mémoire minimale.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"-> Le dossier '{local_path}' n'existe pas. Début du téléchargement de '{prefix}'...\")\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        try:\n",
    "            objects = client.list_objects(bucket, prefix=prefix, recursive=True)\n",
    "            files_to_download = [obj for obj in objects if not obj.object_name.endswith('/')]\n",
    "\n",
    "            if not files_to_download:\n",
    "                print(f\"   AVERTISSEMENT : Aucun fichier trouvé sur MinIO avec le préfixe '{prefix}'.\")\n",
    "                return\n",
    "\n",
    "            for obj in files_to_download:\n",
    "                file_name = os.path.relpath(obj.object_name, prefix)\n",
    "                if file_name == 'consolidated.safetensors':\n",
    "                    file_name = 'model.safetensors'\n",
    "                \n",
    "                local_file_path = os.path.join(local_path, file_name)\n",
    "                \n",
    "                if not os.path.exists(os.path.dirname(local_file_path)):\n",
    "                    os.makedirs(os.path.dirname(local_file_path))\n",
    "                \n",
    "                print(f\"   Téléchargement en streaming de '{obj.object_name}'...\")\n",
    "                \n",
    "                response = None\n",
    "                try:\n",
    "                    response = client.get_object(bucket, obj.object_name)\n",
    "                    with open(local_file_path, 'wb') as file_data:\n",
    "                        for chunk in response.stream(amt=1024*1024):\n",
    "                            file_data.write(chunk)\n",
    "                finally:\n",
    "                    if response:\n",
    "                        response.close()\n",
    "                        response.release_conn()\n",
    "                        \n",
    "            print(f\"   Téléchargement pour '{local_path}' terminé.\")\n",
    "        except S3Error as exc:\n",
    "            print(f\"   Une erreur S3 est survenue pour {prefix}: {exc}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"-> Le modèle dans '{local_path}' existe déjà.\")\n",
    "\n",
    "try:\n",
    "    minio_client = Minio(s3_endpoint, access_key=Access_key, secret_key=Secret_key, secure=False)\n",
    "    print(\"Connexion à MinIO réussie.\")\n",
    "    \n",
    "    download_model_via_streaming(minio_client, MINIO_BUCKET_NAME, LOCAL_MODEL_PATH, LLM_LOCAL_PATH)\n",
    "    download_model_via_streaming(minio_client, MINIO_BUCKET_NAME, EMBEDDING_MINIO_PREFIX, LOCAL_MODEL_PATH_2)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue lors de la phase de téléchargement : {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n Tous les modèles sont vérifiés et prêts localement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702dc6c-1944-4874-a66f-fdfa92cb174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce2700-ab4a-4a6f-b774-5b06e9d07334",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Chargement du tokenizer depuis le chemin local : {LLM_LOCAL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLM_LOCAL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd33f6-d790-4304-91c6-7511ada1c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Chargement du modèle depuis {LLM_LOCAL_PATH} avec quantisation 4-bit...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_LOCAL_PATH,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(\"Modèle chargé avec succès sur le GPU depuis le volume local !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c740f92-82c7-4af1-bcd4-40132a31ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=LOCAL_MODEL_PATH_2, model_kwargs={'device': 'cuda'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f9d9d-2876-437f-84d3-80e6412438bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DOCLING_API = False\n",
    "\n",
    "DOCLING_API_URL = \"http://route-increased-junglefowl-docling.apps.neutron-sno-gpu.neutron-it.fr/v1alpha/convert/file\"\n",
    "PDF_DIRECTORY_ON_MINIO = \"documents/\"\n",
    "LOCAL_PDF_DOWNLOAD_DIR = \"./pdf_downloads/\"\n",
    "\n",
    "all_documents = []\n",
    "os.makedirs(LOCAL_PDF_DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(f\"Recherche des fichiers PDF dans le dossier MinIO: '{PDF_DIRECTORY_ON_MINIO}'...\")\n",
    "    pdf_objects = minio_client.list_objects(MINIO_BUCKET_NAME, prefix=PDF_DIRECTORY_ON_MINIO, recursive=True)\n",
    "    pdf_object_names = [obj.object_name for obj in pdf_objects if obj.object_name.lower().endswith('.pdf')]\n",
    "    print(f\"{len(pdf_object_names)} document(s) PDF trouvé(s) pour traitement.\")\n",
    "\n",
    "    if not pdf_object_names:\n",
    "        raise Exception(\"Aucun fichier PDF n'a été trouvé. Vérifiez le chemin sur MinIO.\")\n",
    "    print(\"-\" * 40)\n",
    "    for pdf_object_name in pdf_object_names:\n",
    "        #print(\"-\" * 40)\n",
    "        #print(f\"Traitement du fichier : '{pdf_object_name}'\")\n",
    "        \n",
    "        local_pdf_path = os.path.join(LOCAL_PDF_DOWNLOAD_DIR, os.path.basename(pdf_object_name))\n",
    "        if not os.path.exists(local_pdf_path):\n",
    "            print(f\"  -> Téléchargement vers '{local_pdf_path}'...\")\n",
    "            minio_client.fget_object(MINIO_BUCKET_NAME, pdf_object_name, local_pdf_path)\n",
    "\n",
    "        if USE_DOCLING_API:\n",
    "            print(\"  -> Méthode choisie : API Docling.\")\n",
    "            with open(local_pdf_path, 'rb') as f:\n",
    "                pdf_content = f.read()\n",
    "            \n",
    "            files_payload = {'files': (os.path.basename(local_pdf_path), pdf_content, 'application/pdf')}\n",
    "            data_payload = {'output_format': 'text'}\n",
    "            \n",
    "            response = requests.post(DOCLING_API_URL, files=files_payload, data=data_payload)\n",
    "            response.raise_for_status()\n",
    "            api_data = response.json()\n",
    "\n",
    "            if 'document' in api_data and 'md_content' in api_data['document'] and api_data['document']['md_content']:\n",
    "                extracted_text = api_data['document']['md_content']\n",
    "                all_documents.append(Document(page_content=extracted_text, metadata={\"source\": pdf_object_name}))\n",
    "                print(f\"  -> Texte extrait avec succès pour '{pdf_object_name}'.\")\n",
    "            else:\n",
    "                print(f\"  -> ERREUR : La réponse de l'API pour '{pdf_object_name}' est invalide. Fichier ignoré.\")\n",
    "        else:\n",
    "            #print(\"  -> Méthode choisie : Traitement local avec PyPDFLoader.\")\n",
    "            loader = PyPDFLoader(local_pdf_path)\n",
    "            documents_from_this_pdf = loader.load()\n",
    "            all_documents.extend(documents_from_this_pdf)\n",
    "            #print(f\"  -> Document chargé localement. {len(documents_from_this_pdf)} pages ajoutées.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    error_details = e.response.text if hasattr(e, 'response') else str(e)\n",
    "    raise Exception(f\"Une erreur est survenue lors du traitement des PDF : {error_details}\")\n",
    "\n",
    "if not all_documents:\n",
    "    raise Exception(\"Aucun document n'a pu être traité. Arrêt du script.\")\n",
    "    \n",
    "print(f\"Traitement terminé. Nombre total de documents/pages prêt(s) pour le RAG : {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055a932-51eb-4550-8551-7fb4708237c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Découpage des documents en chunks sémantiques...\")\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "all_chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Nombre total de chunks créés : {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871789e-b66a-4001-a9ab-2e7709e207ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initialisation du Vector Store (ChromaDB) avec les chunks...\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_chunks, embedding=embeddings)\n",
    "\n",
    "print(\"Vector Store prêt !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2bd35-251b-4455-a362-e877bc3e09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Un retriever qui trouve le meilleur document et retourne ce document\n",
    "    plus son voisin d'avant et son voisin d'après.\n",
    "    \"\"\"\n",
    "    vectorstore: Chroma\n",
    "    all_docs: List[Document]\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        # A. On trouve le meilleur chunk correspondant à la question\n",
    "        best_docs = self.vectorstore.similarity_search(query, k=1)\n",
    "        if not best_docs:\n",
    "            return []\n",
    "\n",
    "        best_doc_content = best_docs[0].page_content\n",
    "        \n",
    "        # B. On retrouve son index dans la liste complète des chunks\n",
    "        try:\n",
    "            best_doc_index = [doc.page_content for doc in self.all_docs].index(best_doc_content)\n",
    "        except ValueError:\n",
    "            # Si on ne le trouve pas (très rare), on retourne juste le meilleur doc\n",
    "            return best_docs\n",
    "\n",
    "        # C. On définit la \"fenêtre\" : 1 avant, 1 après\n",
    "        start_index = max(0, best_doc_index - 1)\n",
    "        end_index = min(len(self.all_docs), best_doc_index + 2) # +2 car la tranche est exclusive\n",
    "\n",
    "        # D. On retourne les 3 chunks (ou 2 si c'est un bord)\n",
    "        return self.all_docs[start_index:end_index]\n",
    "\n",
    "# --- 4. On crée une instance de notre nouveau retriever ---\n",
    "print(\"Initialisation du retriever personnalisé 'Voisins'...\")\n",
    "retriever = NeighborRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    all_docs=all_chunks\n",
    ")\n",
    "print(\"Retriever à contexte enrichi (voisins) prêt !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b4f9d-a6df-455a-9c85-60b5ed9be550",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=4096,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_full_text=False\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b0b8f-5fad-4c1b-8c58-39477317e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = ChatPromptTemplate.from_template(\"\"\"[INST] Tu es un assistant qui répond uniquement en te basant sur le CONTEXTE FOURNI.\n",
    "#Lis attentivement le contexte avant de répondre.\n",
    "#Si la réponse à la question de l'utilisateur NE SE TROUVE PAS dans le contexte, réponds UNIQUEMENT 'Je ne peux pas répondre à cette question avec le contexte donné.'\n",
    "#NE RÉPONDS JAMAIS sur la base de tes connaissances préalables.\n",
    "\n",
    "#Contexte:\n",
    "#{context}\n",
    "\n",
    "#Question: {input} [/INST]\"\"\")\n",
    "\n",
    "prompt_template_str = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Tu es un assistant utile et bienveillant. Ton objectif est de fournir des réponses complètes et précises.\n",
    "\n",
    "Pour répondre à la question de l'utilisateur, réfère-toi d'abord au CONTEXTE FOURNI.\n",
    "Si la réponse NE SE TROUVE PAS dans le contexte, utilise alors tes propres connaissances pour y répondre.\n",
    "Si tu ne connais pas la réponse, qu'elle soit dans le contexte ou non, dis simplement que tu ne sais pas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "def format_llama_direct_prompt(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07962b86-4d99-4c37-ae1c-6926eac96018",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81affadc-a13f-4b3e-a36c-7da0802795e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1454c1-d893-4c78-820b-529a2b472c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query_with_llm(question: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Utilise le LLM pour générer plusieurs reformulations fidèles de la question de l'utilisateur.\n",
    "    \"\"\"\n",
    "    transformation_prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Tu es un outil de reformulation de requêtes. Ton seul rôle est de réécrire la question de l'utilisateur pour la rendre optimale pour une recherche sémantique.\n",
    "    Règles strictes:\n",
    "    1. Ne réponds JAMAIS à la question.\n",
    "    2. Garde le sens original de la question.\n",
    "    3. Ta sortie doit être une et une seule question.\n",
    "    4. La question reformulée doit être concise.\n",
    "    \n",
    "    Exemple 1:\n",
    "    Utilisateur: infos sur la sécurité openshift\n",
    "    Assistant: Quelles sont les meilleures pratiques de sécurité pour un cluster OpenShift ?\n",
    "    \n",
    "    Exemple 2:\n",
    "    Utilisateur: tu peux me faire un résumé ?\n",
    "    Assistant: Quel est le résumé du document fourni ?\n",
    "    \n",
    "    Exemple 3:\n",
    "    Utilisateur: c'est quoi les grands titre dont tu peux m'aider, en se basant sur le contexte?\n",
    "    Assistant: Quels sont les thèmes principaux abordés dans le document ?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    formatted_prompt = transformation_prompt_template.format(question=question)\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    new_tokens = outputs[0][input_length:]\n",
    "    transformed_question = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return transformed_question.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c53ee8-62a1-4361-89dc-884e227e0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_llama_direct_prompt(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cceed-4adf-44ab-b04a-f1e668913116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n Système RAG prêt. Posez une question (tapez 'exit' pour quitter) :\")\n",
    "while True:\n",
    "    try:\n",
    "        prompt_text = input(\">>> \")\n",
    "        if prompt_text.lower() == 'exit':\n",
    "            print(\"Fermeture.\")\n",
    "            break\n",
    "            \n",
    "        print(\"\\n--- Transformation de la question par le LLM ---\")\n",
    "        transformed_query = transform_query_with_llm(prompt_text, model, tokenizer)\n",
    "        print(f\"Question transformée : '{transformed_query}'\")\n",
    "        \n",
    "        print(\"\\n--- Réponse du Modèle Llama (Connaissances Générales) ---\")\n",
    "        \n",
    "        direct_prompt_formatted = format_llama_direct_prompt(transformed_query)\n",
    "        \n",
    "        encodeds_direct = tokenizer(direct_prompt_formatted, return_tensors=\"pt\")\n",
    "        model_inputs_direct = encodeds_direct.to(model.device)\n",
    "\n",
    "        outputs_direct = model.generate(\n",
    "            input_ids=model_inputs_direct.input_ids,\n",
    "            attention_mask=model_inputs_direct.attention_mask,\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        input_length = model_inputs_direct.input_ids.shape[1]\n",
    "        new_tokens = outputs_direct[0][input_length:]\n",
    "        answer_direct = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Modèle Llama (Direct) : {answer_direct.strip()}\")\n",
    "\n",
    "        print(\"\\n--- Réponse du RAG (Basée sur le Contexte du Fichier) ---\")\n",
    "        response_rag = retrieval_chain.invoke({\"input\": transformed_query})\n",
    "        print(f\"Modèle RAG : {response_rag['answer'].strip()}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nFermeture.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1565f3-bb81-4a9d-bd75-99166445edbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
